{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the Cohere dataset form Hugging face\n",
    "# cohere_dataset = datasets.load_dataset(\"Cohere/wikipedia-22-12\", 'en')\n",
    "\n",
    "# as The dataset is very big. So I have downloaded a protion of it to work on it.\n",
    "\n",
    "cohere_dataset = load_dataset ('json', data_files = r\"C:\\Users\\Z004RJZU\\Documents\\LLM_Practice\\LLM\\Datasets\\Cohere\\000.jsonl\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['wiki_id', 'url', 'views', 'langs', 'title', 'text', 'paragraph_id', 'id'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohere_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wiki_id': [69407798, 3524766, 3524766],\n",
       " 'url': ['https://en.wikipedia.org/wiki?curid=69407798',\n",
       "  'https://en.wikipedia.org/wiki?curid=3524766',\n",
       "  'https://en.wikipedia.org/wiki?curid=3524766'],\n",
       " 'views': [5674.4492597435465, 5409.5609619796405, 5409.5609619796405],\n",
       " 'langs': [38, 184, 184],\n",
       " 'title': ['Deaths in 2022', 'YouTube', 'YouTube'],\n",
       " 'text': ['The following notable deaths occurred in 2022. Names are reported under the date of death, in alphabetical order. A typical entry reports information in the following sequence:',\n",
       "  'YouTube is a global online video sharing and social media platform headquartered in San Bruno, California. It was launched on February 14, 2005, by Steve Chen, Chad Hurley, and Jawed Karim. It is owned by Google, and is the second most visited website, after Google Search. YouTube has more than 2.5 billion monthly users who collectively watch more than one billion hours of videos each day. , videos were being uploaded at a rate of more than 500 hours of content per minute.',\n",
       "  \"In October 2006, YouTube was bought by Google for $1.65\\xa0billion. Google's ownership of YouTube expanded the site's business model, expanding from generating revenue from advertisements alone, to offering paid content such as movies and exclusive content produced by YouTube. It also offers YouTube Premium, a paid subscription option for watching content without ads. YouTube also approved creators to participate in Google's AdSense program, which seeks to generate more revenue for both parties. YouTube reported revenue of $19.8 billion in 2020. In 2021, YouTube's annual advertising revenue increased to $28.8 billion.\"],\n",
       " 'paragraph_id': [0, 0, 1],\n",
       " 'id': [0, 1, 2]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohere_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'title', 'text'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing unnecessory columns from dataset\n",
    "cohere_dataset = cohere_dataset.select_columns ([ 'url',  'title', 'text' ])\n",
    "cohere_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'title', 'text', 'text_len'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohere_dataset_text_len = cohere_dataset.map(lambda x : {'text_len' : len(x['text']) })\n",
    "cohere_dataset_text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://en.wikipedia.org/wiki?curid=69407798',\n",
       " 'title': 'Deaths in 2022',\n",
       " 'text': 'The following notable deaths occurred in 2022. Names are reported under the date of death, in alphabetical order. A typical entry reports information in the following sequence:',\n",
       " 'text_len': 176}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohere_dataset_text_len[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a581d2bc493e4e62a5a0e006256b31af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_comp'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmb_dataset = cohere_dataset_text_len. map (lambda x : {'text_comp' :  x[ 'title' ] + \" \\n \" + x [ 'text' ] + \" \\n \" + x [ 'url' ] }, ).select_columns('text_comp')\n",
    "cmb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_comp': 'Deaths in 2022 \\n The following notable deaths occurred in 2022. Names are reported under the date of death, in alphabetical order. A typical entry reports information in the following sequence: \\n https://en.wikipedia.org/wiki?curid=69407798'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmb_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True in (np.array( cohere_dataset_text_len['text_len']) < 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As not text is small then 25 words so I dont need to remove small paragraphs from dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tockenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained (model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding \n",
    "datacollector =  DataCollatorWithPadding (model_ckpt, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    encoded_text = tokenizer ( text=text, truncation=True,padding=True, return_tensors='pt' ).to(device)\n",
    "    model.to (device)\n",
    "    encodeed_last_out_state = model(** encoded_text).last_hidden_state[:,0]\n",
    "    return encodeed_last_out_state\n",
    "\n",
    "    \n",
    "\n",
    "data = get_embedding ( cohere_dataset_text_len['text'][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9356e-01,  3.2731e-01, -2.3691e-01,  3.4945e-01, -1.0965e-01,\n",
       "         -2.7030e-01,  3.6670e-02,  4.9790e-02, -2.8614e-01,  2.8275e-01,\n",
       "          2.5917e-01,  2.7235e-01,  1.5068e-01, -1.3791e-01, -1.4966e-02,\n",
       "         -4.1428e-01,  3.3530e-01, -1.3885e-01, -2.5672e-01,  2.2994e-01,\n",
       "         -2.8966e-01,  4.6551e-01, -3.0376e-01, -1.1332e-01, -9.9217e-02,\n",
       "          2.6644e-01, -3.4776e-02,  1.5970e-01,  1.4803e-01, -3.3811e-01,\n",
       "         -2.6734e-01, -1.5671e-01,  1.9524e-04, -1.8536e-01, -1.0521e-04,\n",
       "         -3.2258e-01,  3.0876e-01, -8.5313e-02, -3.4510e-01,  9.0497e-02,\n",
       "         -3.4652e-01, -2.5919e-01, -1.5309e-01, -2.6753e-01,  4.1939e-03,\n",
       "         -4.6443e-01, -1.1720e-01, -1.0840e-02,  9.7699e-02,  5.7875e-02,\n",
       "          2.8734e-01, -2.1094e-01,  4.7801e-01, -3.7467e-01,  1.9660e-01,\n",
       "          2.3358e-02, -2.2511e-01, -1.6760e-01,  1.5551e-01, -2.6801e-01,\n",
       "         -4.3818e-02,  2.8158e-01,  1.9379e-02, -1.3384e-01,  3.2284e-01,\n",
       "         -1.8912e-03, -2.0879e-03,  2.0857e-01,  2.5066e-01,  1.9896e-01,\n",
       "          2.8953e-01, -2.0607e-01,  2.4655e-01, -1.4143e-01, -2.1065e-01,\n",
       "         -2.2746e-01,  3.8458e-01, -1.0679e-01,  2.8674e-01, -2.5936e-01,\n",
       "          2.1069e-01,  4.8410e-01, -1.4185e-01, -3.5386e-02, -2.1223e-01,\n",
       "          8.2360e-02, -1.3592e-01,  8.1278e-02, -2.5868e-01, -3.1586e-01,\n",
       "          1.6325e-01,  2.3523e-01,  3.1977e-01,  5.6421e-02, -1.8868e-01,\n",
       "         -1.6160e-02,  1.3571e-01,  2.4240e-01,  1.0217e-01, -1.4950e-01,\n",
       "         -1.4188e-02, -1.2241e-01, -1.7109e-01,  7.9538e-02,  2.7679e-01,\n",
       "         -1.9305e-01,  3.3073e-01, -2.8448e-01,  1.1988e-01,  1.9123e-01,\n",
       "          3.0061e-01, -8.4447e-03,  2.8852e-02,  7.5270e-02,  1.5493e-01,\n",
       "          1.9087e-01,  1.9602e-01, -4.3835e-01, -4.0943e-01,  3.2888e-01,\n",
       "         -1.8067e-01,  1.6490e-02, -3.4345e-01,  1.2350e-01, -2.6895e-01,\n",
       "          1.3972e-01, -4.6520e-02, -3.5111e-01,  1.4786e-01,  3.5045e-02,\n",
       "         -2.4421e-01,  7.0673e-02, -5.9561e-02,  4.8156e-02,  8.5873e-02,\n",
       "         -2.7183e-02,  1.2669e-01,  1.1782e-01,  3.1411e-01, -2.0852e-01,\n",
       "          1.3169e-02, -1.7195e-01,  2.0135e-01,  3.0640e-01, -3.3123e-02,\n",
       "         -1.7548e-01,  4.1406e-02,  2.7872e-02, -1.5404e-02,  1.6411e-01,\n",
       "          9.2566e-02, -1.1766e-01,  1.1358e-01,  3.1012e-01, -5.5842e-02,\n",
       "         -1.3078e-01,  3.9132e-01, -1.0621e-02,  2.2287e-01, -3.8641e-01,\n",
       "          8.7626e-02,  3.0576e-01, -3.0136e-01, -3.4374e-01,  2.3830e-01,\n",
       "          1.1230e-01, -4.3296e-01,  1.5452e-01, -1.5782e-01,  1.1177e-01,\n",
       "          3.2033e-01, -4.4323e-02,  1.6144e-01, -1.7625e-02, -2.1582e-02,\n",
       "         -1.0177e-01, -4.5504e-01, -7.0828e-02,  3.1019e-01, -2.6995e-01,\n",
       "          2.0872e-02, -1.0927e-01,  1.4169e-01,  2.7639e-02,  3.2705e-01,\n",
       "         -3.4303e-01,  4.3912e-01, -2.9884e-01, -1.0171e-01, -5.6337e-02,\n",
       "          1.1464e-01, -1.1364e-01,  1.0071e-01,  1.7467e-01, -3.6534e-03,\n",
       "         -8.0491e-02,  1.7252e-01,  1.5997e-01,  4.4595e-02,  1.7751e-01,\n",
       "         -7.0874e-02,  7.6000e-02,  1.8505e-01,  2.7518e-01,  5.9422e-01,\n",
       "          7.9207e-02, -7.4055e-02,  1.9000e-01,  2.9660e-01, -1.3942e-01,\n",
       "          2.6203e-02,  2.6769e-01, -4.8669e-02,  1.8048e-01,  7.9946e-02,\n",
       "          3.6737e-01,  2.9423e-01, -2.5989e-01,  6.2479e-02,  3.4947e-02,\n",
       "         -2.2732e-01,  2.4972e-01, -5.3385e-01, -1.1194e-01, -1.3656e-02,\n",
       "          1.3543e-01,  1.3847e-01, -1.5186e-01, -2.5138e-02,  2.8248e-01,\n",
       "          6.0127e-01,  7.6968e-02,  1.0418e-02,  8.2176e-02,  1.1695e-01,\n",
       "          2.7854e-01, -6.6647e-03,  3.7134e-01,  4.7547e-02, -4.6489e-01,\n",
       "          1.3583e-01,  5.7120e-02, -2.2698e-01, -1.3348e-01, -2.5999e-01,\n",
       "          2.2016e-01, -7.0729e-03,  1.7440e-01, -9.7484e-02,  1.4777e-01,\n",
       "         -8.4065e-02, -1.0184e-01,  1.4864e-01, -1.9321e-01,  4.2069e-01,\n",
       "         -1.0841e-01, -2.4457e-01,  1.6945e-01, -2.5743e-01,  1.4027e-01,\n",
       "         -6.0148e-02, -5.4175e-02, -7.5742e-02,  6.0045e-02, -1.8945e-01,\n",
       "         -3.7291e-02,  1.9712e-01,  7.9960e-02,  2.9581e-01,  5.8142e-02,\n",
       "         -1.1965e-01, -1.6571e-01,  1.0754e-01, -3.1545e-01,  1.2802e-01,\n",
       "         -4.6495e-02, -2.0016e-01,  2.7338e-01,  1.7448e-01,  1.2506e-01,\n",
       "         -1.2110e-01,  2.2183e-01,  2.2710e-01,  1.2724e-01, -1.7089e-01,\n",
       "         -2.9867e-01,  5.7324e-02, -3.7276e-01,  1.9848e-01,  2.3145e-01,\n",
       "          2.3359e-01,  4.2461e-02,  2.6715e-01,  2.0920e-03, -5.2683e-02,\n",
       "         -4.8140e-02, -5.7539e-02,  3.9047e-01, -1.8952e-01,  3.7688e-01,\n",
       "         -2.1918e-01,  2.1008e-01,  1.4218e-01,  3.8098e-01, -6.2383e-01,\n",
       "          3.1411e-04,  4.8781e-01, -1.2662e-01, -2.7911e-02, -3.3841e-01,\n",
       "         -4.5334e-01,  1.0084e-01, -1.0341e-01, -3.7346e-01,  1.6182e-01,\n",
       "          7.2675e-02,  6.7001e-02, -9.1218e-02,  7.8640e-02, -1.8045e-01,\n",
       "         -1.7190e-01,  4.8814e-02, -3.3285e-01,  4.0613e-01, -1.5390e-01,\n",
       "          4.3643e-02,  1.7300e-02, -1.1645e-01,  1.4946e-03,  1.7782e-01,\n",
       "          6.9885e-02, -1.4513e-01, -5.0626e-01, -1.4945e-01,  9.6008e-02,\n",
       "         -5.3290e-02, -1.0611e-01, -6.6689e-02,  6.8772e-02, -1.7662e-01,\n",
       "          3.8657e-02,  9.3608e-02,  2.7433e-01, -3.2674e-01, -6.4051e-02,\n",
       "         -1.3197e-01, -4.7451e-01,  9.3566e-02, -8.2755e-02, -6.8255e-02,\n",
       "         -3.5418e-01,  4.1733e-01,  1.5829e-01, -3.1683e-01, -3.3090e-01,\n",
       "         -2.3408e-01,  1.7445e-01,  3.1373e-01,  3.9628e-01, -2.5295e-02,\n",
       "          3.7466e-01,  1.5611e-01,  3.9735e-01,  2.0230e-01,  2.2835e-01,\n",
       "         -1.1647e-01,  1.9268e-01, -1.4873e-01,  3.0692e-01,  2.3857e-01,\n",
       "          1.3492e-01, -1.4437e-01,  9.7416e-02,  4.4709e-02,  3.0036e-01,\n",
       "         -1.2277e-01, -9.8220e-03,  2.1920e-01, -3.6386e-01, -7.0697e-02,\n",
       "         -1.7532e-02,  3.1155e-01,  8.0397e-01, -2.1718e-01, -2.0076e-01,\n",
       "         -2.2619e-01, -4.3675e-01,  4.1714e-02,  1.1895e-01,  2.6150e-01,\n",
       "         -1.2724e-01, -1.4122e-02, -1.9398e-02, -8.0907e-02,  1.7841e-01,\n",
       "         -6.1501e-02, -9.8140e-02, -4.5327e-01, -2.3149e-01,  1.0009e-01,\n",
       "          3.4883e-02, -2.8890e-01,  1.5859e-01,  1.4011e-01,  1.5590e-01,\n",
       "          3.5984e-02,  1.8016e-01, -6.0691e-02, -1.2766e-01,  6.3953e-02,\n",
       "          1.7069e-02,  1.7703e-01, -2.0365e-01, -3.9877e-02,  4.7248e-01,\n",
       "         -2.7554e-02, -1.2734e-01, -4.9836e-01,  3.4446e-02,  6.9636e-02,\n",
       "         -3.0540e-02, -2.2951e-01,  1.8091e-01, -4.2598e-01, -3.3816e-01,\n",
       "         -2.1825e-01,  2.7512e-01,  7.3768e-03,  2.7538e-01, -1.2140e-01,\n",
       "          1.3321e-01,  2.3723e-01, -2.9145e-02,  5.9113e-01,  1.3971e-02,\n",
       "          1.1847e-01, -1.4154e-01,  1.0054e-01, -3.4110e-01, -1.6847e-01,\n",
       "          7.9588e-02,  1.1884e-01,  2.9057e-01, -2.8391e-01,  4.4557e-01,\n",
       "          2.2814e-01, -3.9332e-03, -6.7356e-02, -3.1585e-01,  1.0924e-01,\n",
       "         -2.3649e-01,  2.2854e-02, -1.8429e-01,  4.3929e-01,  1.5292e-01,\n",
       "         -3.2261e-01,  1.9502e-01, -1.2520e-01, -1.3854e-01, -9.1462e-02,\n",
       "         -1.8763e-01,  7.4847e-01, -7.2665e-02, -1.3148e-01,  3.3298e-02,\n",
       "          7.5654e-02,  1.2851e-01,  2.7016e-03, -1.3985e-01, -1.1067e-01,\n",
       "         -7.0703e-03, -1.3804e-03, -1.4101e-01, -6.4907e-02, -1.7591e-01,\n",
       "         -1.2170e-01,  2.1897e-01, -4.2168e-01,  1.4102e-03, -3.7535e-02,\n",
       "         -2.3392e-01,  5.4967e-02, -5.8356e-02, -4.2272e-02,  2.1060e-01,\n",
       "          5.5985e-02, -2.1942e-01, -8.7990e-02, -5.6999e-02,  9.9412e-02,\n",
       "          4.4349e-02,  1.6122e-01, -5.0346e-02, -1.9665e-01,  6.8658e-02,\n",
       "          2.0378e-01, -3.9608e-01, -7.8047e-02,  2.1244e-01,  3.5160e-01,\n",
       "         -2.5973e-02,  2.0953e-01,  3.7465e-02,  2.1325e-01,  1.2516e-01,\n",
       "          1.6635e-01,  2.2064e-01, -3.0310e-01, -1.8532e-01, -9.9836e-02,\n",
       "          3.9111e-02, -2.1489e-01, -5.2876e-01, -3.3383e-01,  1.3577e-01,\n",
       "          2.2397e-02, -1.3073e-02,  1.1717e-01,  3.3319e-02, -3.4764e-01,\n",
       "         -4.0383e-02,  3.0702e-01,  4.8838e-01,  5.5583e-03,  2.7258e-01,\n",
       "         -4.1738e-01,  7.6855e-02, -1.2914e-01,  6.0572e-02,  6.3492e-02,\n",
       "          2.4224e-01,  3.6551e-02,  3.7156e-01,  6.7395e-02, -3.5243e-02,\n",
       "         -2.7766e-01, -1.8098e-01, -2.8013e-02, -1.3161e-01,  2.0456e-01,\n",
       "          1.5926e-01,  2.9156e-02,  5.7324e-01,  1.5955e-01, -2.6132e-01,\n",
       "         -3.2686e-01, -1.3292e-01, -1.4973e-02,  3.8811e-01,  2.0543e-01,\n",
       "         -3.0052e-01, -1.7508e-01,  8.2476e-02, -3.0983e-01, -1.5220e-01,\n",
       "         -3.9574e-01,  2.1278e-02, -1.6197e-01,  2.2509e-01, -1.1532e-01,\n",
       "         -3.4373e-01, -6.7851e-02,  6.7111e-02,  2.4323e-01, -6.0058e-02,\n",
       "          2.5983e-02, -3.1110e-01,  6.8354e-02,  7.5228e-02, -9.5516e-02,\n",
       "          1.1826e-01, -4.3283e-01, -3.7042e-01, -5.7754e-02,  2.8107e-02,\n",
       "          7.6596e-02, -1.7863e-01,  1.7340e-01, -1.1018e-01, -1.9039e-01,\n",
       "          2.3844e-01,  3.2735e-01,  1.0061e-01,  1.4726e-01,  1.4558e-01,\n",
       "         -4.1259e-01,  5.1334e-02, -1.5173e-01,  2.2916e-02,  2.6762e-01,\n",
       "          1.3365e-01, -2.0869e-01, -3.4645e-01,  1.3416e-01,  1.1943e-01,\n",
       "         -3.0322e-01,  3.4096e-01,  1.1469e-01,  1.5091e-01, -1.2104e-01,\n",
       "          7.3292e-03, -2.9963e-01,  3.3397e-01, -1.2204e-01,  9.2437e-02,\n",
       "         -5.6857e-02,  4.5705e-01,  9.1540e-02, -5.6977e-02,  2.6871e-01,\n",
       "         -2.4890e-01,  7.9740e-02,  1.3829e-01,  2.4338e-01, -4.7421e-01,\n",
       "         -2.2176e-01,  7.1451e-03,  4.2554e-01,  3.4907e-01, -5.3540e-02,\n",
       "          6.7335e-02,  3.2689e-02,  3.0248e-01,  5.9910e-01,  2.3362e-01,\n",
       "          3.8118e-01,  2.6902e-01,  5.5493e-01, -1.6141e-01,  2.8686e-01,\n",
       "          2.8243e-01, -1.9916e-01, -1.0688e-01,  3.0907e-01,  2.1943e-01,\n",
       "         -1.0986e-01,  2.4481e-01, -4.1628e-01, -2.4209e-01,  9.8573e-02,\n",
       "          3.5164e-01,  1.7573e-01, -1.4531e-01,  2.0838e-01, -3.2506e-01,\n",
       "         -1.1723e-02, -2.2149e-03,  2.2992e-01, -6.1266e-02, -2.5292e-02,\n",
       "         -3.1982e-01, -3.9014e-02,  3.5762e-01,  4.3544e-01, -2.1008e-01,\n",
       "          4.9981e-02,  2.3411e-01, -2.2074e-01,  1.1019e-01,  1.9408e-01,\n",
       "          2.0830e-01, -2.1685e-01,  1.0187e-01,  2.0186e-01, -2.9602e-01,\n",
       "         -2.2375e-01,  1.5533e-01,  9.6899e-02,  1.9062e-01, -6.0630e-02,\n",
       "         -1.9958e-01,  2.6754e-01, -2.1728e-01, -2.8283e-01,  1.2373e-01,\n",
       "          8.8530e-02,  6.6975e-02,  5.7161e-02, -4.0564e-01, -1.1417e-01,\n",
       "          2.6830e-02, -9.9025e-02, -9.5323e-02, -1.2741e-02,  2.4459e-01,\n",
       "          3.9275e-01, -5.5545e-02,  1.2689e-01,  1.5847e-01,  8.9775e-02,\n",
       "         -3.7180e-01,  1.9491e-02,  2.4982e-01, -5.2730e-03, -8.3691e-01,\n",
       "         -1.8782e-01,  2.2297e-01,  5.5914e-02,  3.0560e-01,  2.0884e-01,\n",
       "         -6.1916e-02,  2.7217e-01,  9.6425e-02, -2.8992e-01, -2.7652e-01,\n",
       "         -9.6273e-02,  1.5213e-01, -6.4655e-01,  1.0776e-02,  2.4338e-01,\n",
       "          1.0461e-01, -3.0772e-02,  1.9859e-01,  1.5110e-01,  5.6935e-02,\n",
       "          1.3446e-01, -3.3118e-01, -3.4374e-01, -4.3099e-02,  4.4076e-01,\n",
       "          1.1284e-01,  9.5470e-03,  2.8425e-01, -7.4960e-02, -2.4127e-01,\n",
       "          4.1013e-01, -1.1838e-01, -2.6864e-03,  9.9555e-02,  1.4056e-02,\n",
       "         -3.3301e-02,  8.7438e-02, -1.2452e-01,  3.5839e-01,  2.6113e-01,\n",
       "         -2.2234e-01, -4.0443e-01, -4.6414e-01, -1.0355e-01,  3.3960e-02,\n",
       "          4.6087e-02, -4.4502e-01,  1.0914e-01,  3.2122e-02, -2.1509e-01,\n",
       "         -4.9663e-02, -2.8259e-01,  1.8348e-01, -2.5399e-01, -6.1313e-02,\n",
       "         -4.8660e-01,  1.6350e-01, -1.7475e-01,  7.8832e-02,  2.5832e-01,\n",
       "         -1.3266e-01,  2.5388e-01, -1.4762e-01,  2.8784e-01,  1.2053e-01,\n",
       "         -4.7107e-02, -2.3399e-03, -1.1828e-01, -1.1202e-01, -1.2202e-03,\n",
       "         -1.9203e-02, -2.1608e-01, -1.0398e-01]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below work is done to check what kind of functionality does a fucntion provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  2000,  2210,  3866,  6681,  4162,  2003, 16802,  2479,  1016,\n",
       "          3419,  2028,  2992,  2108,  2000,  3062,  2001,  2335,  1014,  2003,\n",
       "         12444,  7480,  2348,  1016,  1041,  5175,  4447,  4315,  2596,  2003,\n",
       "          2000,  2210,  5541,  1028,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = cohere_dataset_text_len['text'][0]\n",
    "encoded_text_0 = tokenizer ( text=text, truncation=True,padding=True, return_tensors='pt' ).to(device=device)\n",
    "encoded_text_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.9356e-01,  3.2731e-01, -2.3691e-01,  3.4945e-01, -1.0965e-01,\n",
       "          -2.7030e-01,  3.6670e-02,  4.9790e-02, -2.8614e-01,  2.8275e-01,\n",
       "           2.5917e-01,  2.7235e-01,  1.5068e-01, -1.3791e-01, -1.4966e-02,\n",
       "          -4.1428e-01,  3.3530e-01, -1.3885e-01, -2.5672e-01,  2.2994e-01,\n",
       "          -2.8966e-01,  4.6551e-01, -3.0376e-01, -1.1332e-01, -9.9217e-02,\n",
       "           2.6644e-01, -3.4776e-02,  1.5970e-01,  1.4803e-01, -3.3811e-01,\n",
       "          -2.6734e-01, -1.5671e-01,  1.9524e-04, -1.8536e-01, -1.0521e-04,\n",
       "          -3.2258e-01,  3.0876e-01, -8.5313e-02, -3.4510e-01,  9.0497e-02,\n",
       "          -3.4652e-01, -2.5919e-01, -1.5309e-01, -2.6753e-01,  4.1939e-03,\n",
       "          -4.6443e-01, -1.1720e-01, -1.0840e-02,  9.7699e-02,  5.7875e-02,\n",
       "           2.8734e-01, -2.1094e-01,  4.7801e-01, -3.7467e-01,  1.9660e-01,\n",
       "           2.3358e-02, -2.2511e-01, -1.6760e-01,  1.5551e-01, -2.6801e-01,\n",
       "          -4.3818e-02,  2.8158e-01,  1.9379e-02, -1.3384e-01,  3.2284e-01,\n",
       "          -1.8912e-03, -2.0879e-03,  2.0857e-01,  2.5066e-01,  1.9896e-01,\n",
       "           2.8953e-01, -2.0607e-01,  2.4655e-01, -1.4143e-01, -2.1065e-01,\n",
       "          -2.2746e-01,  3.8458e-01, -1.0679e-01,  2.8674e-01, -2.5936e-01,\n",
       "           2.1069e-01,  4.8410e-01, -1.4185e-01, -3.5386e-02, -2.1223e-01,\n",
       "           8.2360e-02, -1.3592e-01,  8.1278e-02, -2.5868e-01, -3.1586e-01,\n",
       "           1.6325e-01,  2.3523e-01,  3.1977e-01,  5.6421e-02, -1.8868e-01,\n",
       "          -1.6160e-02,  1.3571e-01,  2.4240e-01,  1.0217e-01, -1.4950e-01,\n",
       "          -1.4188e-02, -1.2241e-01, -1.7109e-01,  7.9538e-02,  2.7679e-01,\n",
       "          -1.9305e-01,  3.3073e-01, -2.8448e-01,  1.1988e-01,  1.9123e-01,\n",
       "           3.0061e-01, -8.4447e-03,  2.8852e-02,  7.5270e-02,  1.5493e-01,\n",
       "           1.9087e-01,  1.9602e-01, -4.3835e-01, -4.0943e-01,  3.2888e-01,\n",
       "          -1.8067e-01,  1.6490e-02, -3.4345e-01,  1.2350e-01, -2.6895e-01,\n",
       "           1.3972e-01, -4.6520e-02, -3.5111e-01,  1.4786e-01,  3.5045e-02,\n",
       "          -2.4421e-01,  7.0673e-02, -5.9561e-02,  4.8156e-02,  8.5873e-02,\n",
       "          -2.7183e-02,  1.2669e-01,  1.1782e-01,  3.1411e-01, -2.0852e-01,\n",
       "           1.3169e-02, -1.7195e-01,  2.0135e-01,  3.0640e-01, -3.3123e-02,\n",
       "          -1.7548e-01,  4.1406e-02,  2.7872e-02, -1.5404e-02,  1.6411e-01,\n",
       "           9.2566e-02, -1.1766e-01,  1.1358e-01,  3.1012e-01, -5.5842e-02,\n",
       "          -1.3078e-01,  3.9132e-01, -1.0621e-02,  2.2287e-01, -3.8641e-01,\n",
       "           8.7626e-02,  3.0576e-01, -3.0136e-01, -3.4374e-01,  2.3830e-01,\n",
       "           1.1230e-01, -4.3296e-01,  1.5452e-01, -1.5782e-01,  1.1177e-01,\n",
       "           3.2033e-01, -4.4323e-02,  1.6144e-01, -1.7625e-02, -2.1582e-02,\n",
       "          -1.0177e-01, -4.5504e-01, -7.0828e-02,  3.1019e-01, -2.6995e-01,\n",
       "           2.0872e-02, -1.0927e-01,  1.4169e-01,  2.7639e-02,  3.2705e-01,\n",
       "          -3.4303e-01,  4.3912e-01, -2.9884e-01, -1.0171e-01, -5.6337e-02,\n",
       "           1.1464e-01, -1.1364e-01,  1.0071e-01,  1.7467e-01, -3.6534e-03,\n",
       "          -8.0491e-02,  1.7252e-01,  1.5997e-01,  4.4595e-02,  1.7751e-01,\n",
       "          -7.0874e-02,  7.6000e-02,  1.8505e-01,  2.7518e-01,  5.9422e-01,\n",
       "           7.9207e-02, -7.4055e-02,  1.9000e-01,  2.9660e-01, -1.3942e-01,\n",
       "           2.6203e-02,  2.6769e-01, -4.8669e-02,  1.8048e-01,  7.9946e-02,\n",
       "           3.6737e-01,  2.9423e-01, -2.5989e-01,  6.2479e-02,  3.4947e-02,\n",
       "          -2.2732e-01,  2.4972e-01, -5.3385e-01, -1.1194e-01, -1.3656e-02,\n",
       "           1.3543e-01,  1.3847e-01, -1.5186e-01, -2.5138e-02,  2.8248e-01,\n",
       "           6.0127e-01,  7.6968e-02,  1.0418e-02,  8.2176e-02,  1.1695e-01,\n",
       "           2.7854e-01, -6.6647e-03,  3.7134e-01,  4.7547e-02, -4.6489e-01,\n",
       "           1.3583e-01,  5.7120e-02, -2.2698e-01, -1.3348e-01, -2.5999e-01,\n",
       "           2.2016e-01, -7.0729e-03,  1.7440e-01, -9.7484e-02,  1.4777e-01,\n",
       "          -8.4065e-02, -1.0184e-01,  1.4864e-01, -1.9321e-01,  4.2069e-01,\n",
       "          -1.0841e-01, -2.4457e-01,  1.6945e-01, -2.5743e-01,  1.4027e-01,\n",
       "          -6.0148e-02, -5.4175e-02, -7.5742e-02,  6.0045e-02, -1.8945e-01,\n",
       "          -3.7291e-02,  1.9712e-01,  7.9960e-02,  2.9581e-01,  5.8142e-02,\n",
       "          -1.1965e-01, -1.6571e-01,  1.0754e-01, -3.1545e-01,  1.2802e-01,\n",
       "          -4.6495e-02, -2.0016e-01,  2.7338e-01,  1.7448e-01,  1.2506e-01,\n",
       "          -1.2110e-01,  2.2183e-01,  2.2710e-01,  1.2724e-01, -1.7089e-01,\n",
       "          -2.9867e-01,  5.7324e-02, -3.7276e-01,  1.9848e-01,  2.3145e-01,\n",
       "           2.3359e-01,  4.2461e-02,  2.6715e-01,  2.0920e-03, -5.2683e-02,\n",
       "          -4.8140e-02, -5.7539e-02,  3.9047e-01, -1.8952e-01,  3.7688e-01,\n",
       "          -2.1918e-01,  2.1008e-01,  1.4218e-01,  3.8098e-01, -6.2383e-01,\n",
       "           3.1411e-04,  4.8781e-01, -1.2662e-01, -2.7911e-02, -3.3841e-01,\n",
       "          -4.5334e-01,  1.0084e-01, -1.0341e-01, -3.7346e-01,  1.6182e-01,\n",
       "           7.2675e-02,  6.7001e-02, -9.1218e-02,  7.8640e-02, -1.8045e-01,\n",
       "          -1.7190e-01,  4.8814e-02, -3.3285e-01,  4.0613e-01, -1.5390e-01,\n",
       "           4.3643e-02,  1.7300e-02, -1.1645e-01,  1.4946e-03,  1.7782e-01,\n",
       "           6.9885e-02, -1.4513e-01, -5.0626e-01, -1.4945e-01,  9.6008e-02,\n",
       "          -5.3290e-02, -1.0611e-01, -6.6689e-02,  6.8772e-02, -1.7662e-01,\n",
       "           3.8657e-02,  9.3608e-02,  2.7433e-01, -3.2674e-01, -6.4051e-02,\n",
       "          -1.3197e-01, -4.7451e-01,  9.3566e-02, -8.2755e-02, -6.8255e-02,\n",
       "          -3.5418e-01,  4.1733e-01,  1.5829e-01, -3.1683e-01, -3.3090e-01,\n",
       "          -2.3408e-01,  1.7445e-01,  3.1373e-01,  3.9628e-01, -2.5295e-02,\n",
       "           3.7466e-01,  1.5611e-01,  3.9735e-01,  2.0230e-01,  2.2835e-01,\n",
       "          -1.1647e-01,  1.9268e-01, -1.4873e-01,  3.0692e-01,  2.3857e-01,\n",
       "           1.3492e-01, -1.4437e-01,  9.7416e-02,  4.4709e-02,  3.0036e-01,\n",
       "          -1.2277e-01, -9.8220e-03,  2.1920e-01, -3.6386e-01, -7.0697e-02,\n",
       "          -1.7532e-02,  3.1155e-01,  8.0397e-01, -2.1718e-01, -2.0076e-01,\n",
       "          -2.2619e-01, -4.3675e-01,  4.1714e-02,  1.1895e-01,  2.6150e-01,\n",
       "          -1.2724e-01, -1.4122e-02, -1.9398e-02, -8.0907e-02,  1.7841e-01,\n",
       "          -6.1501e-02, -9.8140e-02, -4.5327e-01, -2.3149e-01,  1.0009e-01,\n",
       "           3.4883e-02, -2.8890e-01,  1.5859e-01,  1.4011e-01,  1.5590e-01,\n",
       "           3.5984e-02,  1.8016e-01, -6.0691e-02, -1.2766e-01,  6.3953e-02,\n",
       "           1.7069e-02,  1.7703e-01, -2.0365e-01, -3.9877e-02,  4.7248e-01,\n",
       "          -2.7554e-02, -1.2734e-01, -4.9836e-01,  3.4446e-02,  6.9636e-02,\n",
       "          -3.0540e-02, -2.2951e-01,  1.8091e-01, -4.2598e-01, -3.3816e-01,\n",
       "          -2.1825e-01,  2.7512e-01,  7.3768e-03,  2.7538e-01, -1.2140e-01,\n",
       "           1.3321e-01,  2.3723e-01, -2.9145e-02,  5.9113e-01,  1.3971e-02,\n",
       "           1.1847e-01, -1.4154e-01,  1.0054e-01, -3.4110e-01, -1.6847e-01,\n",
       "           7.9588e-02,  1.1884e-01,  2.9057e-01, -2.8391e-01,  4.4557e-01,\n",
       "           2.2814e-01, -3.9332e-03, -6.7356e-02, -3.1585e-01,  1.0924e-01,\n",
       "          -2.3649e-01,  2.2854e-02, -1.8429e-01,  4.3929e-01,  1.5292e-01,\n",
       "          -3.2261e-01,  1.9502e-01, -1.2520e-01, -1.3854e-01, -9.1462e-02,\n",
       "          -1.8763e-01,  7.4847e-01, -7.2665e-02, -1.3148e-01,  3.3298e-02,\n",
       "           7.5654e-02,  1.2851e-01,  2.7016e-03, -1.3985e-01, -1.1067e-01,\n",
       "          -7.0703e-03, -1.3804e-03, -1.4101e-01, -6.4907e-02, -1.7591e-01,\n",
       "          -1.2170e-01,  2.1897e-01, -4.2168e-01,  1.4102e-03, -3.7535e-02,\n",
       "          -2.3392e-01,  5.4967e-02, -5.8356e-02, -4.2272e-02,  2.1060e-01,\n",
       "           5.5985e-02, -2.1942e-01, -8.7990e-02, -5.6999e-02,  9.9412e-02,\n",
       "           4.4349e-02,  1.6122e-01, -5.0346e-02, -1.9665e-01,  6.8658e-02,\n",
       "           2.0378e-01, -3.9608e-01, -7.8047e-02,  2.1244e-01,  3.5160e-01,\n",
       "          -2.5973e-02,  2.0953e-01,  3.7465e-02,  2.1325e-01,  1.2516e-01,\n",
       "           1.6635e-01,  2.2064e-01, -3.0310e-01, -1.8532e-01, -9.9836e-02,\n",
       "           3.9111e-02, -2.1489e-01, -5.2876e-01, -3.3383e-01,  1.3577e-01,\n",
       "           2.2397e-02, -1.3073e-02,  1.1717e-01,  3.3319e-02, -3.4764e-01,\n",
       "          -4.0383e-02,  3.0702e-01,  4.8838e-01,  5.5583e-03,  2.7258e-01,\n",
       "          -4.1738e-01,  7.6855e-02, -1.2914e-01,  6.0572e-02,  6.3492e-02,\n",
       "           2.4224e-01,  3.6551e-02,  3.7156e-01,  6.7395e-02, -3.5243e-02,\n",
       "          -2.7766e-01, -1.8098e-01, -2.8013e-02, -1.3161e-01,  2.0456e-01,\n",
       "           1.5926e-01,  2.9156e-02,  5.7324e-01,  1.5955e-01, -2.6132e-01,\n",
       "          -3.2686e-01, -1.3292e-01, -1.4973e-02,  3.8811e-01,  2.0543e-01,\n",
       "          -3.0052e-01, -1.7508e-01,  8.2476e-02, -3.0983e-01, -1.5220e-01,\n",
       "          -3.9574e-01,  2.1278e-02, -1.6197e-01,  2.2509e-01, -1.1532e-01,\n",
       "          -3.4373e-01, -6.7851e-02,  6.7111e-02,  2.4323e-01, -6.0058e-02,\n",
       "           2.5983e-02, -3.1110e-01,  6.8354e-02,  7.5228e-02, -9.5516e-02,\n",
       "           1.1826e-01, -4.3283e-01, -3.7042e-01, -5.7754e-02,  2.8107e-02,\n",
       "           7.6596e-02, -1.7863e-01,  1.7340e-01, -1.1018e-01, -1.9039e-01,\n",
       "           2.3844e-01,  3.2735e-01,  1.0061e-01,  1.4726e-01,  1.4558e-01,\n",
       "          -4.1259e-01,  5.1334e-02, -1.5173e-01,  2.2916e-02,  2.6762e-01,\n",
       "           1.3365e-01, -2.0869e-01, -3.4645e-01,  1.3416e-01,  1.1943e-01,\n",
       "          -3.0322e-01,  3.4096e-01,  1.1469e-01,  1.5091e-01, -1.2104e-01,\n",
       "           7.3292e-03, -2.9963e-01,  3.3397e-01, -1.2204e-01,  9.2437e-02,\n",
       "          -5.6857e-02,  4.5705e-01,  9.1540e-02, -5.6977e-02,  2.6871e-01,\n",
       "          -2.4890e-01,  7.9740e-02,  1.3829e-01,  2.4338e-01, -4.7421e-01,\n",
       "          -2.2176e-01,  7.1451e-03,  4.2554e-01,  3.4907e-01, -5.3540e-02,\n",
       "           6.7335e-02,  3.2689e-02,  3.0248e-01,  5.9910e-01,  2.3362e-01,\n",
       "           3.8118e-01,  2.6902e-01,  5.5493e-01, -1.6141e-01,  2.8686e-01,\n",
       "           2.8243e-01, -1.9916e-01, -1.0688e-01,  3.0907e-01,  2.1943e-01,\n",
       "          -1.0986e-01,  2.4481e-01, -4.1628e-01, -2.4209e-01,  9.8573e-02,\n",
       "           3.5164e-01,  1.7573e-01, -1.4531e-01,  2.0838e-01, -3.2506e-01,\n",
       "          -1.1723e-02, -2.2149e-03,  2.2992e-01, -6.1266e-02, -2.5292e-02,\n",
       "          -3.1982e-01, -3.9014e-02,  3.5762e-01,  4.3544e-01, -2.1008e-01,\n",
       "           4.9981e-02,  2.3411e-01, -2.2074e-01,  1.1019e-01,  1.9408e-01,\n",
       "           2.0830e-01, -2.1685e-01,  1.0187e-01,  2.0186e-01, -2.9602e-01,\n",
       "          -2.2375e-01,  1.5533e-01,  9.6899e-02,  1.9062e-01, -6.0630e-02,\n",
       "          -1.9958e-01,  2.6754e-01, -2.1728e-01, -2.8283e-01,  1.2373e-01,\n",
       "           8.8530e-02,  6.6975e-02,  5.7161e-02, -4.0564e-01, -1.1417e-01,\n",
       "           2.6830e-02, -9.9025e-02, -9.5323e-02, -1.2741e-02,  2.4459e-01,\n",
       "           3.9275e-01, -5.5545e-02,  1.2689e-01,  1.5847e-01,  8.9775e-02,\n",
       "          -3.7180e-01,  1.9491e-02,  2.4982e-01, -5.2730e-03, -8.3691e-01,\n",
       "          -1.8782e-01,  2.2297e-01,  5.5914e-02,  3.0560e-01,  2.0884e-01,\n",
       "          -6.1916e-02,  2.7217e-01,  9.6425e-02, -2.8992e-01, -2.7652e-01,\n",
       "          -9.6273e-02,  1.5213e-01, -6.4655e-01,  1.0776e-02,  2.4338e-01,\n",
       "           1.0461e-01, -3.0772e-02,  1.9859e-01,  1.5110e-01,  5.6935e-02,\n",
       "           1.3446e-01, -3.3118e-01, -3.4374e-01, -4.3099e-02,  4.4076e-01,\n",
       "           1.1284e-01,  9.5470e-03,  2.8425e-01, -7.4960e-02, -2.4127e-01,\n",
       "           4.1013e-01, -1.1838e-01, -2.6864e-03,  9.9555e-02,  1.4056e-02,\n",
       "          -3.3301e-02,  8.7438e-02, -1.2452e-01,  3.5839e-01,  2.6113e-01,\n",
       "          -2.2234e-01, -4.0443e-01, -4.6414e-01, -1.0355e-01,  3.3960e-02,\n",
       "           4.6087e-02, -4.4502e-01,  1.0914e-01,  3.2122e-02, -2.1509e-01,\n",
       "          -4.9663e-02, -2.8259e-01,  1.8348e-01, -2.5399e-01, -6.1313e-02,\n",
       "          -4.8660e-01,  1.6350e-01, -1.7475e-01,  7.8832e-02,  2.5832e-01,\n",
       "          -1.3266e-01,  2.5388e-01, -1.4762e-01,  2.8784e-01,  1.2053e-01,\n",
       "          -4.7107e-02, -2.3399e-03, -1.1828e-01, -1.1202e-01, -1.2202e-03,\n",
       "          -1.9203e-02, -2.1608e-01, -1.0398e-01]], device='cuda:0',\n",
       "        grad_fn=<SelectBackward0>),\n",
       " torch.Size([1, 768]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to (device)\n",
    "temp = model(**encoded_text_0).last_hidden_state[:,0]\n",
    "temp , temp.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1aa0663742f4648a20c4c16f986da66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.38 GiB already allocated; 0 bytes free; 3.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Z004RJZU\\Documents\\LLM_Practice\\Sementic-Search-with-LLM\\testing.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Sementic-Search-with-LLM/testing.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embedded_dataset \u001b[39m=\u001b[39m cmb_dataset\u001b[39m.\u001b[39;49mmap (\u001b[39mlambda\u001b[39;49;00m x: {\u001b[39m'\u001b[39;49m\u001b[39mtenser\u001b[39;49m\u001b[39m'\u001b[39;49m: get_embedding(x[\u001b[39m'\u001b[39;49m\u001b[39mtext_comp\u001b[39;49m\u001b[39m'\u001b[39;49m] )} )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Sementic-Search-with-LLM/testing.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embedded_dataset\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[0;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[0;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[0;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[0;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[0;32m   3357\u001b[0m     }\n",
      "\u001b[1;32mc:\\Users\\Z004RJZU\\Documents\\LLM_Practice\\Sementic-Search-with-LLM\\testing.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Sementic-Search-with-LLM/testing.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embedded_dataset \u001b[39m=\u001b[39m cmb_dataset\u001b[39m.\u001b[39mmap (\u001b[39mlambda\u001b[39;00m x: {\u001b[39m'\u001b[39m\u001b[39mtenser\u001b[39m\u001b[39m'\u001b[39m: get_embedding(x[\u001b[39m'\u001b[39;49m\u001b[39mtext_comp\u001b[39;49m\u001b[39m'\u001b[39;49m] )} )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Sementic-Search-with-LLM/testing.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embedded_dataset\n",
      "\u001b[1;32mc:\\Users\\Z004RJZU\\Documents\\LLM_Practice\\Sementic-Search-with-LLM\\testing.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Sementic-Search-with-LLM/testing.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m encoded_text \u001b[39m=\u001b[39m tokenizer ( text\u001b[39m=\u001b[39mtext, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m )\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Sementic-Search-with-LLM/testing.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mto (device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Sementic-Search-with-LLM/testing.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m encodeed_last_out_state \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m encoded_text)\u001b[39m.\u001b[39mlast_hidden_state[:,\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Sementic-Search-with-LLM/testing.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m encodeed_last_out_state\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:550\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    549\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids\u001b[39m=\u001b[39minput_ids, position_ids\u001b[39m=\u001b[39mposition_ids, inputs_embeds\u001b[39m=\u001b[39minputs_embeds)\n\u001b[1;32m--> 550\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    551\u001b[0m     embedding_output,\n\u001b[0;32m    552\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    553\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    554\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    555\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    556\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    557\u001b[0m )\n\u001b[0;32m    558\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    559\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:341\u001b[0m, in \u001b[0;36mMPNetEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    339\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[1;32m--> 341\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    342\u001b[0m     hidden_states,\n\u001b[0;32m    343\u001b[0m     attention_mask,\n\u001b[0;32m    344\u001b[0m     head_mask[i],\n\u001b[0;32m    345\u001b[0m     position_bias,\n\u001b[0;32m    346\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    347\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    348\u001b[0m )\n\u001b[0;32m    349\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:310\u001b[0m, in \u001b[0;36mMPNetLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    308\u001b[0m outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    311\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    312\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:264\u001b[0m, in \u001b[0;36mMPNetIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 264\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    265\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.38 GiB already allocated; 0 bytes free; 3.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "embedded_dataset = cmb_dataset.map (lambda x: {'tenser': get_embedding(x['text_comp'] )} )\n",
    "embedded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    encoded_text = tokenizer ( text=text, truncation=True,padding=True, return_tensors='pt' ).to(device)\n",
    "    model.to (device)\n",
    "    encodeed_last_out_state = model(** encoded_text).last_hidden_state[:,0]\n",
    "    return encodeed_last_out_state\n",
    "\n",
    "embedded_dataset = cmb_dataset.map (get_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
